# SPDX-License-Identifier: MIT
# Copyright (C) 2025 RidgeRun, LLC <support@ridgerun.ai>
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation
# files (the “Software”), to deal in the Software without
# restriction, including without limitation the rights to use, copy,
# modify, merge, publish, distribute, sublicense, and/or sell copies
# of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

[property]
##########
# num-detected-classes:
# Number of classes detected by the network
#
# Mandatory
# Integer: >0
# Network type: Detector
# Primary/Secondary: Both
#
# RF-DETR: Pre-trained RF-DETR has 91 classes: 90 objects + background
num-detected-classes=91


##########
# net-scale-factor:
# Pixel normalization factor, according to this formula:
#    y = net-scale-factor * (x[] - offset[])
#
#            x : (int8) Input pixel value. It has range [0,255].
#    offset[c] : (float) Corresponding mean value, read either from the mean
#                file or offsets[c], where c is the channel to which the input
#                pixel belongs, and offsets is the array specified in the
#                configuration file.
#            y : (float) The corresponding output pixel value.
#
# This property is ignored if input-tensor-meta enabled. This depends on the
# normalization factors used to train the model.
#
# Optional (defaults to 1.0)
# Float: > 0.0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: This architecture uses the following (https://github.com/roboflow/rf-detr/blob/0c1e8330e41db310e4d86ddf7a5c1b26e7f67489/rfdetr/detr.py#L49)
#   means = [0.485, 0.456, 0.406]
#   stds = [0.229, 0.224, 0.225]
# Those values are used by torch.normlize (https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize) as
#   output[channel] = (input[channel] - mean[channel]) / std[channel]
#
# We need the following adaptations:
#   1. torch.normalize assumes values are already normalized. NvInfer receives int8 ([0,255])
#   2. torch.normalize uses std, which is net-scale-factor=1/std
#   3. torch.normalize has an std per channel, we have a single net-scale-factor
#
# So:
#   1. Find a single std as the average: 0.226 = (0.229 + 0.224 + 0.225)/3
#   2. Find the inverse of the average std: 1/0.226 = 4.4247787611
#   3. Include the pixel normalization: 4.4247787611/255 = 0.0173520736
# which yields our final result.
#
net-scale-factor=0.0173520736

##########
# model-file:
# Pathname of the model file. Not required if model-engine-file is used
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Ignored in favor of onnx-file and model-engine-file
#model-file=


##########
# proto-file:
# Pathname of the Caffe prototxt file describing the network architecture.
# Not required when using a pre-generated model-engine-file.
#
# Optional
# String (absolute path recommended)
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Not used. Provided only for completeness when handling legacy Caffe models.
#proto-file=


##########
# int8-calib-file:
# Pathname of the INT8 calibration cache used when running an FP32 model in INT8 mode.
# This file encodes dynamic ranges for tensors. Required only when network-mode=1 (INT8)
# and no engine file is provided.
#
# Optional
# String (absolute path recommended)
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Not used in the default configuration
#int8-calib-file=

##########
# batch-size:
# Number of frames or objects that nvinfer processes together in a single batch.
# Impacts memory footprint and throughput. Larger batches increase GPU utilization.
#
# Optional (default=1)
# Integer: > 0
# Network type: All
# Primary/Secondary: Both
batch-size=1

##########
# input-tensor-from-meta:
# Use preprocessed tensors provided as metadata rather than performing preprocessing
# inside nvinfer. When enabled:
#   • nvinfer expects raw tensors matching the model input layout.
#   • batch-size must equal the sum of ROIs generated by gst-nvdspreprocess.
#
# Optional (default=0)
# Boolean
# Network type: All
# Primary/Secondary: Primary only
#
# RF-DETR: No special pre-processing needed. Disabled
input-tensor-from-meta=0

##########
# tensor-meta-pool-size:
# Size of the metadata pool for output tensor meta objects.
# Increase if downstream elements consume tensor metadata extensively.
#
# Optional (default=?)
# Integer: >0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: No need for special configuration
#tensor-meta-pool-size=

##########
# model-engine-file:
# Pathname of a serialized TensorRT engine (.engine).
# When provided, onnx-file/model-file/other builder parameters are ignored.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: based on the onnx name and current configuration
model-engine-file=rfdetr-nano.onnx_b1_gpu0_fp32.engine

##########
# onnx-file:
# Pathname of the ONNX model. Used when building an engine dynamically.
# Required unless model-engine-file is provided.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
onnx-file=rfdetr-nano.onnx

##########
# enable-dbscan:
# Deprecated. Historical toggle for switching between DBSCAN and OpenCV grouping.
# Use cluster-mode instead.
#
# Deprecated
# Boolean
# Network type: Detector
# Primary/Secondary: Both
#
#enable-dbscan=

##########
# labelfile-path:
# Pathname of a plain-text label file. Each line corresponds to a class index.
#
# Optional
# String
# Network type: Detector & Classifier
# Primary/Secondary: Both
#
labelfile-path=coco91_labels.txt

##########
# mean-file:
# Pathname of a PPM-format mean data file. Provides per-pixel mean offsets.
# Ignored if input-tensor-meta is enabled.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: We use the offsets property
#mean-file=

##########
# gie-unique-id:
# Unique ID assigned to this GIE instance. Downstream elements rely on this ID
# for associating output metadata with the correct inference stage.
#
# Optional (default=0)
# Integer: >0
# Network type: All
# Primary/Secondary: Both
#
gie-unique-id=1

##########
# operate-on-gie-id:
# GIE ID whose metadata (bounding boxes / objects) this secondary GIE consumes.
#
# Optional (default=0)
# Integer: >0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Unused, up to the application
#
#operate-on-gie-id=

##########
# operate-on-class-ids:
# Restrict secondary inference to specified parent class-ids.
# Use semicolon-delimited list. -1 means all classes.
#   operate-on-class-ids=1;2
#     Operates on objects with class IDs 1, 2
#   operate-on-class-ids=-1
#     Operates on all classes
#
# Optional
# Semicolon-delimited integers
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Unused, up to the application
#
#operate-on-class-ids=-1

##########
# interval:
# Number of consecutive batches to skip before performing inference again.
# Helps reduce compute load for primary detectors.
#
# Optional (default=0)
# Integer: ≥0
# Network type: All
# Primary/Secondary: Primary only
#
#interval=0

##########
# input-object-min-width / input-object-min-height:
# Minimum object size required for secondary inference. Objects smaller
# than these thresholds are ignored.
#
# Optional (defaults=0)
# Integer ≥0
# Network type: All
# Primary/Secondary: Secondary
#
#input-object-min-width=
#input-object-min-height=

##########
# input-object-max-width / input-object-max-height:
# Maximum allowed object size for secondary inference.
# Set to 0 to disable the constraint.
#
# Optional (defaults=0)
# Integer ≥0
# Network type: All
# Primary/Secondary: Secondary
#
#input-object-max-width=
#input-object-max-height=

##########
# network-mode:
# Precision mode for inference.
#   0 = FP32
#   1 = INT8
#   2 = FP16
#   3 = BEST (TensorRT decides)
#
# Optional (default=0)
# Integer
# Network type: All
# Primary/Secondary: Both
#
network-mode=0

##########
# offsets:
# Mean values to subtract from each channel before applying net-scale-factor.
# Ignored if input-tensor-meta is enabled. See `net-scale-factor` for a more
# detailed explanation.
#
# Optional
# Semicolon-delimited floats ≥0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: This architecture uses the following (https://github.com/roboflow/rf-detr/blob/0c1e8330e41db310e4d86ddf7a5c1b26e7f67489/rfdetr/detr.py#L49)
#   means = [0.485, 0.456, 0.406]
#   stds = [0.229, 0.224, 0.225]
# Those values are used by torch.normlize (https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize) as
#   output[channel] = (input[channel] - mean[channel]) / std[channel]
#
# We need the following adaptations:
#   1. torch.normalize assumes values are already normalized. NvInfer receives int8 ([0,255])
#
# So:
#   1. Denormalize mean values:
#        R:  0.485*255 = 123.675
#        G:  0.456*255 = 116.28
#        B:  0.406*255 = 103.53
# which yields our final result.
offsets=123.675;116.28;103.53

##########
# parse-bbox-func-name:
# Custom bounding box parser function name. Overwrites built-in resnet bbox parsing.
#
# Optional
# String
# Network type: Detector
# Primary/Secondary: Both
#
# RF-DETR: Use our deepstream-rfdetr function
parse-bbox-func-name=deepstream_rfdetr_bbox

##########
# parse-bbox-instance-mask-func-name:
# Custom parsing function for instance segmentation masks.
# Mandatory for instance segmentation networks.
#
# Optional
# String
# Network type: Instance Segmentation
# Primary/Secondary: Primary
#
# RF-DETR: Unused, only detection is supported as per now
#parse-bbox-instance-mask-func-name=

##########
# custom-lib-path:
# Absolute pathname to a custom library containing parser implementations or
# custom TensorRT engine creation logic.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
custom-lib-path=libdeepstream-rfdetr.so

##########
# model-color-format:
# Specifies expected color input order.
#   0 = RGB
#   1 = BGR
#   2 = GRAY
#
# Optional (default=0)
# Integer
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Trained on RGB
model-color-format=0

##########
# classifier-async-mode:
# Runs classifier inference asynchronously and attaches results to subsequent
# buffers. Requires tracker IDs to be present.
#
# Optional (default=false)
# Boolean
# Network type: Classifier
# Primary/Secondary: Secondary
#
# RF-DETR: Unused for us
#classifier-async-mode=


##########
# process-mode:
# Execution mode: primary (1) or secondary (2).
# Ignored when input-tensor-meta is enabled.
#
# Optional (default=1)
# Integer
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Up to the application
process-mode=1

##########
# classifier-threshold:
# Minimum probability required to output a classification label.
#
# Mandatory for classifiers
# Float ≥0
# Network type: Classifier
# Primary/Secondary: Both
#
classifier-threshold=0.4

##########
# secondary-reinfer-interval:
# Minimum number of frames before re-running secondary inference on the same object.
#
# Optional (default=0)
# Integer ≥0
# Network type: Detector & Classifier
# Primary/Secondary: Secondary
#
# RF-DETR: Unused, up to the application
#secondary-reinfer-interval=

##########
# output-tensor-meta:
# Attach raw output tensors from the network as metadata.
#
# Optional (default=0)
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Not needed
#output-tensor-meta=


##########
# output-instance-mask:
# Attach instance segmentation masks to output metadata.
#
# Optional
# Boolean
# Network type: Instance Segmentation
# Primary/Secondary: Primary
#
# RF-DETR: Segmentation is not supported yet
#output-instance-mask=

##########
# enable-dla:
# Enable DLA hardware execution on supported Jetson platforms.
#
# Optional
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Not tested yet
enable-dla=0

##########
# use-dla-core:
# Select DLA core index.
#
# Optional
# Integer ≥0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: DLA is not tested yet
#use-dla-core=

##########
# network-type:
# Type of network implemented by the model.
#   0 = Detector
#   1 = Classifier
#   2 = Segmentation
#   3 = Instance Segmentation
#
# Mandatory
# Integer
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: This is a detector
network-type=0

##########
# maintain-aspect-ratio:
# Maintain aspect ratio when scaling frames or object crops to network resolution.
# When enabled, padding is added to preserve aspect ratio.
#
# Optional (default=0)
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: This will yield better results
maintain-aspect-ratio=1

##########
# symmetric-padding:
# Use symmetric padding when maintain-aspect-ratio=1.
# DeepStream uses asymmetric padding by default.
#
# Optional (default=0)
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: shouldn't make much of a difference, but the RF-DETR code pads symmetrically
symmetric-padding=1

##########
# parse-classifier-func-name:
# Custom classifier output parsing function.
# If not set, internal softmax-based parsing is used.
#
# Optional
# String
# Network type: Classifier
# Primary/Secondary: Both
#
# RF-DETR: Unused by rf-detr
#parse-classifier-func-name=

##########
# custom-network-config:
# Pathname of a custom network configuration file used by the custom
# engine creation interface.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: We dont use a special engine, ignored
#custom-network-config=

##########
# tlt-encoded-model:
# Pathname of a TAO (formerly TLT) encoded model.
# Used together with tlt-model-key for decryption and engine generation.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
#tlt-encoded-model=


##########
# tlt-model-key:
# Decryption key for TAO encoded models.
#
# Mandatory when tlt-encoded-model is used
# String
# Network type: All
# Primary/Secondary: Both
#
#tlt-model-key=

##########
# segmentation-threshold:
# Pixel-wise confidence threshold for segmentation models.
# Pixels with confidence below this threshold are assigned class -1.
#
# Optional (default=0.0)
# Float ≥0.0
# Network type: Segmentation, Instance Segmentation
# Primary/Secondary: Both
#
# RF-DETR: ignored, segmentation is not supported yet
#segmentation-threshold=0.3


##########
# segmentation-output-order:
# Output tensor layout for segmentation networks.
#   0 = NCHW
#   1 = NHWC
#
# Optional (default=0)
# Integer
# Network type: Segmentation
# Primary/Secondary: Both
#
# RF-DETR: ignored, segmentation is not supported yet
#segmentation-output-order=0

##########
# workspace-size:
# TensorRT builder workspace size in MB.
# Higher values may enable more aggressive optimizations.
#
# Optional (builder-dependent default)
# Integer >0
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: We use the default
#workspace-size=


##########
# force-implicit-batch-dim:
# Force implicit batch dimension mode when the network supports both
# implicit and explicit batch modes.
#
# Optional (default=?)
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: unused by rfdetr
#force-implicit-batch-dim=

##########
# engine-create-func-name:
# Name of the custom function used to create a TensorRT ICudaEngine.
# Implemented in the custom-lib-path library.
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Unused by rfdetr
#engine-create-func-name=

##########
# cluster-mode:
# Clustering algorithm used to group raw detector outputs.
#   0 = OpenCV groupRectangles()
#   1 = DBSCAN
#   2 = Non-Maximum Suppression (NMS)
#   3 = DBSCAN + NMS hybrid
#   4 = No clustering (for instance segmentation too)
#
# Optional (default=0)
# Integer
# Network type: Detector
# Primary/Secondary: Both
#
# RF-DETR: Doesnt need a clustering method
cluster-mode=4

##########
# filter-out-class-ids:
# Filter out objects that belong to the specified class-ids.
# Use a semicolon-delimited list.
#
# Optional
# Semicolon-delimited integer array
# Network type: Detector
# Primary/Secondary: Both
#
#filter-out-class-ids=

##########
# scaling-filter:
# Filter used when scaling frames / object crops to the network input resolution.
# Ignored when input-tensor-meta is enabled.
# Values correspond to NvBufSurfTransform_Inter in nvbufsurftransform.h.
#
# Optional (platform default)
# Integer
# Network type: All
# Primary/Secondary: Both
#
#scaling-filter=

##########
# scaling-compute-hw:
# Hardware used for scaling operations.
#   0 = Platform default (GPU on dGPU, VIC on Jetson)
#   1 = GPU
#   2 = VIC (Jetson only)
#
# Optional (default=0)
# Integer
# Network type: All
# Primary/Secondary: Both
#
scaling-compute-hw=0

##########
# output-io-formats:
# Per-output configuration of data type and tensor order.
# Format:
#   <layer-name>:<binding-index>:<data-type>:<order>[;...]
#
# data-type ∈ {fp32, fp16, int32, int8}
# order ∈ {chw, chw2, chw4, hwc8, chw16, chw32}
#
# Optional (default: FP32 + CHW for unspecified layers)
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: unused, default works fine
#output-io-formats=

##########
# layer-device-precision:
# Per-layer precision and device configuration.
# Format:
#   <layer-name>:<precision>:<device-type>[;...]
#
# precision in {fp32, fp16, int8}
# device-type in {gpu, dla}
#
# Optional
# String
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: unused, default works fine
#layer-device-precision=

##########
# network-input-order:
# Input layout for the network.
# Ignored when input-tensor-meta is enabled.
#   0 = NCHW
#   1 = NHWC
#
# Optional (default=0)
# Integer
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: we need NCHW
network-input-order=0

##########
# classifier-type:
# Identifier describing the classifier’s function.
# Allowed characters: alphanumeric, '-' and '_', no spaces.
#
# Optional
# String
# Network type: Classifier
# Primary/Secondary: Both
#
#classifier-type=

##########
# crop-objects-to-roi-boundary:
# Clip object bounding boxes to the specified ROI boundary when present.
#
# Optional (default=0)
# Boolean
# Network type: All
# Primary/Secondary: Both
#
# RF-DETR: Unused by rfdetr
#crop-objects-to-roi-boundary=0


#######################################################

[class-attrs-all]

##########
# pre-cluster-threshold:
# Detection confidence threshold applied before clustering.
# Helps prune extremely low-confidence proposals prior to grouping.
#
# Optional (default=?)
# Float ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Filtered by our custom bbox parsing library
pre-cluster-threshold=0.5


##########
# post-cluster-threshold:
# Confidence threshold applied after clustering. Useful for filtering clusters
# that merge multiple weak proposals.
#
# Optional (default=0.0)
# Float ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: rfdetr has no clustering stage, unused
#post-cluster-threshold=0.5


##########
# eps:
# Epsilon distance threshold:
#   • For OpenCV groupRectangles(): rectangle proximity threshold.
#   • For DBSCAN: maximum distance between neighbors for density clustering.
#
# Optional (default varies by algorithm)
# Float ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Unused by RF-DETR
#eps=0


##########
# group-threshold:
# Required number of grouped rectangles for OpenCV groupRectangles().
# Values:
#   • ≥1 enable rectangle grouping.
#   • 0 disables grouping entirely.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Unused by RF-DETR
#group-threshold=


##########
# minBoxes:
# Minimum number of points required to form a DBSCAN cluster.
# Values:
#   • ≥1 enable DBSCAN clustering.
#   • 0 disables clustering entirely.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Unused by RF-DETR
#minBoxes=


##########
# dbscan-min-score:
# Minimum sum of confidence scores for all neighbors in a DBSCAN cluster.
# Ensures only clusters formed from sufficiently strong proposals are kept.
#
# Optional (default=0.0)
# Float ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Unused by RF-DETR
#dbscan-min-score=


##########
# nms-iou-threshold:
# Intersection-over-Union (IOU) threshold used by Non-Maximum Suppression.
# If two bounding boxes exceed this IOU, the one with lower confidence is removed.
#
# Optional (default=0.0)
# Float ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
# RF-DETR: Unused by RF-DETR
#nms-iou-threshold=


##########
# roi-top-offset:
# Excludes objects above this offset from the top of the frame.
# Use to restrict detections to a vertical region of interest.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
roi-top-offset=0


##########
# roi-bottom-offset:
# Excludes objects below this offset from the bottom of the frame.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
roi-bottom-offset=0


##########
# detected-min-w:
# Minimum allowed output width for detected objects.
# Objects smaller than this width are removed.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
#detected-min-w=


##########
# detected-min-h:
# Minimum allowed output height for detected objects.
# Objects smaller than this height are removed.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
#detected-min-h=


##########
# detected-max-w:
# Maximum allowed output width. Objects wider than this are removed.
# Set to 0 to disable the constraint.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
#detected-max-w=


##########
# detected-max-h:
# Maximum allowed output height. Objects taller than this are removed.
# Set to 0 to disable the constraint.
#
# Optional (default=0)
# Integer ≥0
# Network type: Object Detector
# Primary/Secondary: Both
#
#detected-max-h=


##########
# topk:
# Keep only the top K detections sorted by confidence.
# Values:
#   • ≥0 restricts to K detections
#   • -1 disables and keeps all detections
#
# Optional (default=-1)
# Integer ≥-1
# Network type: Object Detector
# Primary/Secondary: Both
#
#topk=-1
